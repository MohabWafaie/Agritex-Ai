{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88de8922-7e42-45d2-bbe4-dd388afb3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from thop import profile\n",
    "import torchvision.datasets\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d92e168-4e11-4d58-9c6f-94edfc44c01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|████████████████████████████████████████| 1143/1143 [09:55<00:00,  1.92it/s, accuracy=78, loss=0.0276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1891, Accuracy: 96.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|█████████████████████████████████████| 1143/1143 [09:52<00:00,  1.93it/s, accuracy=95.1, loss=0.00515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0633, Accuracy: 98.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|█████████████████████████████████████| 1143/1143 [09:51<00:00,  1.93it/s, accuracy=97.2, loss=0.00262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0378, Accuracy: 99.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|█████████████████████████████████████| 1143/1143 [09:51<00:00,  1.93it/s, accuracy=98.1, loss=0.00173]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0297, Accuracy: 99.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|█████████████████████████████████████| 1143/1143 [09:51<00:00,  1.93it/s, accuracy=98.5, loss=0.00131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0254, Accuracy: 99.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|████████████████████████████████████| 1143/1143 [09:51<00:00,  1.93it/s, accuracy=98.9, loss=0.000968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0228, Accuracy: 99.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|████████████████████████████████████| 1143/1143 [09:52<00:00,  1.93it/s, accuracy=99.1, loss=0.000789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0214, Accuracy: 99.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|████████████████████████████████████| 1143/1143 [09:53<00:00,  1.93it/s, accuracy=99.3, loss=0.000629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0199, Accuracy: 99.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|████████████████████████████████████| 1143/1143 [09:51<00:00,  1.93it/s, accuracy=99.4, loss=0.000557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0193, Accuracy: 99.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|████████████████████████████████████| 1143/1143 [09:51<00:00,  1.93it/s, accuracy=99.5, loss=0.00044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0200, Accuracy: 99.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|███████████████████████████████████| 1143/1143 [09:51<00:00,  1.93it/s, accuracy=99.6, loss=0.000379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0192, Accuracy: 99.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|███████████████████████████████████| 1143/1143 [09:51<00:00,  1.93it/s, accuracy=99.6, loss=0.000333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0194, Accuracy: 99.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|███████████████████████████████████| 1143/1143 [09:52<00:00,  1.93it/s, accuracy=99.7, loss=0.000282]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0192, Accuracy: 99.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|███████████████████████████████████| 1143/1143 [09:53<00:00,  1.93it/s, accuracy=99.8, loss=0.000244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0201, Accuracy: 99.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|███████████████████████████████████| 1143/1143 [09:52<00:00,  1.93it/s, accuracy=99.7, loss=0.000232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0191, Accuracy: 99.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|███████████████████████████████████| 1143/1143 [09:52<00:00,  1.93it/s, accuracy=99.8, loss=0.000169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0196, Accuracy: 99.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|███████████████████████████████████| 1143/1143 [09:52<00:00,  1.93it/s, accuracy=99.8, loss=0.000188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0211, Accuracy: 99.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|███████████████████████████████████| 1143/1143 [09:52<00:00,  1.93it/s, accuracy=99.8, loss=0.000156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0211, Accuracy: 99.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|███████████████████████████████████| 1143/1143 [09:52<00:00,  1.93it/s, accuracy=99.9, loss=0.000143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0204, Accuracy: 99.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|███████████████████████████████████| 1143/1143 [09:53<00:00,  1.93it/s, accuracy=99.8, loss=0.000139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0199, Accuracy: 99.69%\n",
      "Validation loss did not improve for 5 consecutive epochs. Early stopping...\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_size = datasets.ImageFolder(root=r\"C:\\Users\\aliof\\OneDrive\\Desktop\\faculity project\\processed resized data\\augmented ali data\\balanced_dataset\\train\", transform=transform)\n",
    "val_size = datasets.ImageFolder(root=r\"C:\\Users\\aliof\\OneDrive\\Desktop\\faculity project\\processed resized data\\augmented ali data\\balanced_dataset\\valid\", transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_size, batch_size=42, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=42, shuffle=False)\n",
    "\n",
    "# Load pre-trained EfficientNet model\n",
    "base_model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=len(train_size.classes))\n",
    "\n",
    "# Optionally, move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), lr=0.00001)\n",
    "num_epochs = 50\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    base_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True) as t:\n",
    "        for inputs, labels in t:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = base_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            t.set_postfix(loss=running_loss / total, accuracy=100 * correct / total)\n",
    "            \n",
    "    # Validation loop\n",
    "    base_model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = base_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(base_model.state_dict(), 'best_model_paper.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f'Validation loss did not improve for {patience} consecutive epochs. Early stopping...')\n",
    "        break\n",
    "\n",
    "print('Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3fe8a63-2260-4185-b32c-0593033cad39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: Class Label - Common wheat\n",
      "Index 1: Class Label - Convolvulus arvensis\n",
      "Index 2: Class Label - Cotton\n",
      "Index 3: Class Label - Euphorbia peplus\n",
      "Index 4: Class Label - Grass\n",
      "Index 5: Class Label - Lolium multiflorum\n",
      "Index 6: Class Label - Maize\n",
      "Index 7: Class Label - Nutgrass\n",
      "Index 8: Class Label - Purslane\n",
      "Index 9: Class Label - Sesame\n",
      "Index 10: Class Label - Sugar beet\n",
      "Index 11: Class Label - Tomato\n"
     ]
    }
   ],
   "source": [
    "train_size = datasets.ImageFolder(root=r\"C:\\Users\\aliof\\OneDrive\\Desktop\\faculity project\\processed resized data\\augmented ali data\\balanced_dataset\\train\", transform=transform)\n",
    "# Get class labels\n",
    "class_labels = train_size.classes\n",
    "\n",
    "# Print class labels with their index\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Index {i}: Class Label - {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72047d6d-a8ca-40e5-9e41-5bcafcecb2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "For EfficientNet:\n",
      "FLOPS: 27.03 million\n",
      "Number of parameters: 4.02 million\n",
      "Number of trainable parameters: 4.02 million\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained EfficientNet model\n",
    "base_model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=12)\n",
    "\n",
    "# Load trained model's state dictionary\n",
    "state_dict = torch.load('best_model_paper.pth')\n",
    "\n",
    "# Load state dictionary into the model's parameters\n",
    "base_model.load_state_dict(state_dict)\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "\n",
    "# Define input tensor shape (batch_size, channels, height, width)\n",
    "input_shape = torch.randn(1, 3, 224, 224).to(device)  # Assuming input images are RGB with size 224x224\n",
    "\n",
    "# Count the number of parameters\n",
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "\n",
    "# Calculate FLOPS\n",
    "flops, _ = profile(base_model, inputs=(input_shape,), verbose=False)\n",
    "\n",
    "print(\"For EfficientNet:\")\n",
    "print(f\"FLOPS: {flops / (10**6):.2f} million\")\n",
    "print(f\"Number of parameters: {total_params / (10**6):.2f} million\")\n",
    "print(f\"Number of trainable parameters: {trainable_params / (10**6):.2f} million\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "115c57a5-25de-4a3e-8dfc-e73b2e50ea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  5.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class Common wheat: 90.91%\n",
      "Accuracy for class Convolvulus arvensis: 100.00%\n",
      "Accuracy for class Cotton: 75.00%\n",
      "Accuracy for class Euphorbia peplus: 21.43%\n",
      "Accuracy for class Grass: 100.00%\n",
      "Accuracy for class Lolium multiflorum: 100.00%\n",
      "Accuracy for class Maize: 75.00%\n",
      "Accuracy for class Nutgrass: 96.43%\n",
      "Accuracy for class Purslane: 100.00%\n",
      "Accuracy for class Sesame: 100.00%\n",
      "Accuracy for class Sugar beet: 75.00%\n",
      "Accuracy for class Tomato: 42.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "])\n",
    "\n",
    "class_labels = train_size.classes\n",
    "\n",
    "# Load dataset\n",
    "val_size = datasets.ImageFolder(root=r\"C:\\Users\\aliof\\OneDrive\\Desktop\\faculity project\\test\\New folder\", transform=transform)\n",
    "\n",
    "# Create data loader for validation set\n",
    "val_loader = DataLoader(val_size, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pre-trained EfficientNet model\n",
    "base_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "\n",
    "# Replace the classifier of the EfficientNet model with a new one matching the number of classes\n",
    "num_classes = len(class_labels)\n",
    "num_ftrs = base_model._fc.in_features\n",
    "base_model._fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Load trained model\n",
    "base_model.load_state_dict(torch.load('best_model_paper.pth'))\n",
    "\n",
    "# Optionally, move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "base_model.to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "base_model.eval()\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = base_model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if label == pred:\n",
    "                class_correct[label] += 1\n",
    "            class_total[label] += 1\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = [class_correct[i] / class_total[i] if class_total[i] != 0 else 0 for i in range(num_classes)]\n",
    "for i in range(num_classes):\n",
    "    print(f\"Accuracy for class {class_labels[i]}: {class_accuracy[i] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95bbd45d-14be-4ffe-b235-d43c562a4234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for class Common wheat:\n",
      "  Precision: 1.00\n",
      "  Recall: 0.91\n",
      "  F1-score: 0.95\n",
      "Metrics for class Convolvulus arvensis:\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1-score: 1.00\n",
      "Metrics for class Cotton:\n",
      "  Precision: 0.38\n",
      "  Recall: 0.75\n",
      "  F1-score: 0.50\n",
      "Metrics for class Euphorbia peplus:\n",
      "  Precision: 1.00\n",
      "  Recall: 0.21\n",
      "  F1-score: 0.35\n",
      "Metrics for class Grass:\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1-score: 1.00\n",
      "Metrics for class Lolium multiflorum:\n",
      "  Precision: 0.97\n",
      "  Recall: 1.00\n",
      "  F1-score: 0.98\n",
      "Metrics for class Maize:\n",
      "  Precision: 0.75\n",
      "  Recall: 0.75\n",
      "  F1-score: 0.75\n",
      "Metrics for class Nutgrass:\n",
      "  Precision: 0.96\n",
      "  Recall: 0.96\n",
      "  F1-score: 0.96\n",
      "Metrics for class Purslane:\n",
      "  Precision: 0.40\n",
      "  Recall: 1.00\n",
      "  F1-score: 0.57\n",
      "Metrics for class Sesame:\n",
      "  Precision: 0.91\n",
      "  Recall: 1.00\n",
      "  F1-score: 0.95\n",
      "Metrics for class Sugar beet:\n",
      "  Precision: 0.30\n",
      "  Recall: 0.75\n",
      "  F1-score: 0.43\n",
      "Metrics for class Tomato:\n",
      "  Precision: 1.00\n",
      "  Recall: 0.43\n",
      "  F1-score: 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define lists to store metrics\n",
    "class_precision = []\n",
    "class_recall = []\n",
    "class_f1_score = []\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "base_model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = base_model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate precision, recall, and F1-score for each class\n",
    "for i in range(num_classes):\n",
    "    precision = precision_score(true_labels, predictions, labels=[i], average='micro')\n",
    "    recall = recall_score(true_labels, predictions, labels=[i], average='micro')\n",
    "    f1 = f1_score(true_labels, predictions, labels=[i], average='micro')\n",
    "    class_precision.append(precision)\n",
    "    class_recall.append(recall)\n",
    "    class_f1_score.append(f1)\n",
    "    print(f\"Metrics for class {class_labels[i]}:\")\n",
    "    print(f\"  Precision: {precision:.2f}\")\n",
    "    print(f\"  Recall: {recall:.2f}\")\n",
    "    print(f\"  F1-score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69e64177-d6dc-4a74-bb16-7794e69ab12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Predicted Class: euphorbia\n",
      "Probability: 0.9999508857727051\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformation for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the trained model\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=11)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"C:\\Users\\aliof\\OneDrive\\Desktop\\tomatocut\\0a9f2d5a-bc30-c5fd-4d6d-dc0b720e4b52.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Apply transformations\n",
    "input_image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_image = input_image.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "class_labels = ['Lolium_multiflorum', 'Maize', 'Nutgrass', 'Purslane', 'Convolvulus_arvensis', \n",
    "                'cotton', 'euphorbia', 'sesame', 'sugarbeet', 'tomato', 'wheat']\n",
    "\n",
    "# Print prediction\n",
    "print(\"Predicted Class:\", class_labels[predicted_class])\n",
    "print(\"Probability:\", probabilities[predicted_class].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce8029-559b-41b3-bd90-860e882a9f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
