{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0395bc86-1c6a-489b-ade2-799490660a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torchsummary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from thop import profile\n",
    "import torchvision.datasets\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb1ad1a6-94d1-445f-bb47-fc4e304a97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "])\n",
    "train_size = datasets.ImageFolder(root=r\"D:\\Projects\\Weed Detection\\resized_dataset\\train\", transform=transform)\n",
    "val_size = datasets.ImageFolder(root=r\"D:\\Projects\\Weed Detection\\resized_dataset\\valid\", transform=transform)\n",
    "test_size = datasets.ImageFolder(root=r\"D:\\Projects\\Weed Detection\\resized_dataset\\test\", transform=transform)\n",
    "train_loader = DataLoader(train_size, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=10, shuffle=False)\n",
    "test_loader = DataLoader(val_size, batch_size=10, shuffle=False)\n",
    "num_epochs = 20\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f21dcce-da80-4656-95f7-cbdcc75e64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(w, k=3, s=1, p=0, m=True):\n",
    "    \"\"\"\n",
    "    Returns the right size of the flattened tensor after\n",
    "        convolutional transformation\n",
    "    :param w: width of image\n",
    "    :param k: kernel size\n",
    "    :param s: stride\n",
    "    :param p: padding\n",
    "    :param m: max pooling (bool)\n",
    "    :return: proper shape and params: use x * x * previous_out_channels\n",
    "\n",
    "    Example:\n",
    "    r = flatten(*flatten(*flatten(w=100, k=3, s=1, p=0, m=True)))[0]\n",
    "    self.fc1 = nn.Linear(r*r*128, 1024)\n",
    "    \"\"\"\n",
    "    return int((np.floor((w - k + 2 * p) / s) + 1) / (2 if m else 1)), k, s, p, m\n",
    "r = flatten(*flatten(*flatten(w=100, k=3, s=1, p=0, m=True)))[0]\n",
    "x= flatten(*flatten(*flatten(*flatten(224, 3, 1, 0, True))))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a2d7e7-b210-44d1-b138-b53b5c995cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=3, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=0)\n",
    "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0)\n",
    "        # Add dropout layer for regularization (optional)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.flat = nn.Flatten()\n",
    "        # Define fully-connected layers\n",
    "        self.fc1 = nn.Linear(in_features=9216, out_features=64)\n",
    "        self.fc2 = nn.Linear(in_features=64, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers with activation and pooling\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(nn.functional.relu(self.conv3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(nn.functional.relu(self.conv4(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.flat(x)\n",
    "        # Apply fully-connected layers with activation\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75df31b8-7a39-423d-a192-fc035cba5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters (adjust as needed)\n",
    "input_channels = 3  # Assuming RGB images\n",
    "num_classes = 12  # Number of output classes\n",
    "\n",
    "# Create an instance of the model\n",
    "model = CNNModel(input_channels, num_classes)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Define loss function and optimizer (choose appropriate ones for your task)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a45488f-cd58-48b4-af27-84384a77c4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 222, 222]             448\n",
      "         MaxPool2d-2         [-1, 16, 111, 111]               0\n",
      "           Dropout-3         [-1, 16, 111, 111]               0\n",
      "            Conv2d-4         [-1, 32, 109, 109]           4,640\n",
      "         MaxPool2d-5           [-1, 32, 54, 54]               0\n",
      "           Dropout-6           [-1, 32, 54, 54]               0\n",
      "            Conv2d-7           [-1, 32, 52, 52]           9,248\n",
      "         MaxPool2d-8           [-1, 32, 26, 26]               0\n",
      "           Dropout-9           [-1, 32, 26, 26]               0\n",
      "           Conv2d-10           [-1, 64, 24, 24]          18,496\n",
      "        MaxPool2d-11           [-1, 64, 12, 12]               0\n",
      "          Dropout-12           [-1, 64, 12, 12]               0\n",
      "          Flatten-13                 [-1, 9216]               0\n",
      "           Linear-14                   [-1, 64]         589,888\n",
      "           Linear-15                   [-1, 12]             780\n",
      "================================================================\n",
      "Total params: 623,500\n",
      "Trainable params: 623,500\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 14.83\n",
      "Params size (MB): 2.38\n",
      "Estimated Total Size (MB): 17.78\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "summary = torchsummary.summary(model, input_size=(3, 224, 224))  # Replace with your input size\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37cc6d5c-9bd9-4841-b0c7-d5e607dd9ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████████████████████████████████| 1125/1125 [01:24<00:00, 13.28it/s, accuracy=80.7, loss=0.0174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3578, Accuracy: 89.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████████████████████████████████| 1125/1125 [01:23<00:00, 13.45it/s, accuracy=86.2, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3245, Accuracy: 88.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|█████████████████████████████████████| 1125/1125 [01:22<00:00, 13.60it/s, accuracy=89.3, loss=0.00992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2525, Accuracy: 92.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████████████████████████████████| 1125/1125 [01:22<00:00, 13.59it/s, accuracy=87.3, loss=0.0117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2646, Accuracy: 91.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|█████████████████████████████████████| 1125/1125 [01:23<00:00, 13.51it/s, accuracy=92.4, loss=0.00719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1962, Accuracy: 93.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|█████████████████████████████████████| 1125/1125 [01:23<00:00, 13.51it/s, accuracy=93.3, loss=0.00622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2088, Accuracy: 93.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|████████████████████████████████████████| 1125/1125 [01:23<00:00, 13.54it/s, accuracy=94, loss=0.0057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1880, Accuracy: 93.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|█████████████████████████████████████| 1125/1125 [01:24<00:00, 13.29it/s, accuracy=94.2, loss=0.00531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1918, Accuracy: 94.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|█████████████████████████████████████| 1125/1125 [01:25<00:00, 13.22it/s, accuracy=94.9, loss=0.00466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1854, Accuracy: 94.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|████████████████████████████████████| 1125/1125 [01:23<00:00, 13.41it/s, accuracy=95.2, loss=0.00439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2105, Accuracy: 93.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|█████████████████████████████████████| 1125/1125 [01:23<00:00, 13.41it/s, accuracy=95.5, loss=0.0042]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1942, Accuracy: 94.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|████████████████████████████████████| 1125/1125 [01:23<00:00, 13.40it/s, accuracy=95.7, loss=0.00396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1855, Accuracy: 94.81%\n",
      "Validation loss did not improve for 3 consecutive epochs. Early stopping...\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True) as t:\n",
    "        for inputs, labels in t:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            t.set_postfix(loss=running_loss / total, accuracy=100 * correct / total)\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "    if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f'Validation loss did not improve for {patience} consecutive epochs. Early stopping...')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c091a1f-86fd-4648-bed5-5025f2677b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Common wheat',\n",
       " 'Convolvulus arvensis',\n",
       " 'Cotton',\n",
       " 'Euphorbia peplus',\n",
       " 'Grass',\n",
       " 'Lolium multiflorum',\n",
       " 'Maize',\n",
       " 'Nutgrass',\n",
       " 'Purslane',\n",
       " 'Sesame',\n",
       " 'Sugar beet',\n",
       " 'Tomato']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a635e2d9-010b-46fb-abc8-12cb277d2c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 391/391 [01:02<00:00,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class Common wheat: 92.50%\n",
      "Accuracy for class Convolvulus arvensis: 98.88%\n",
      "Accuracy for class Cotton: 91.85%\n",
      "Accuracy for class Euphorbia peplus: 99.28%\n",
      "Accuracy for class Grass: 100.00%\n",
      "Accuracy for class Lolium multiflorum: 88.66%\n",
      "Accuracy for class Maize: 70.78%\n",
      "Accuracy for class Nutgrass: 81.21%\n",
      "Accuracy for class Purslane: 68.46%\n",
      "Accuracy for class Sesame: 94.02%\n",
      "Accuracy for class Sugar beet: 89.21%\n",
      "Accuracy for class Tomato: 93.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "])\n",
    "\n",
    "class_labels = ['Common wheat',\n",
    " 'Convolvulus arvensis',\n",
    " 'Cotton',\n",
    " 'Euphorbia peplus',\n",
    " 'Grass',\n",
    " 'Lolium multiflorum',\n",
    " 'Maize',\n",
    " 'Nutgrass',\n",
    " 'Purslane',\n",
    " 'Sesame',\n",
    " 'Sugar beet',\n",
    " 'Tomato']\n",
    "# Load dataset\n",
    "val_size = datasets.ImageFolder(root=r\"D:\\Projects\\Weed Detection\\balanced_dataset\\test\", transform=transform)\n",
    "\n",
    "# Create data loader for validation set\n",
    "val_loader = DataLoader(val_size, batch_size=10, shuffle=False)\n",
    "\n",
    "model = CNNModel(input_channels, num_classes)\n",
    "\n",
    "# Define loss function and optimizer (choose appropriate ones for your task)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# Replace the classifier of the EfficientNet model with a new one matching the number of classes\n",
    "num_classes = len(class_labels)\n",
    "# num_ftrs = model._fc.in_features\n",
    "# model._fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Load trained model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Optionally, move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "model.eval()\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if label == pred:\n",
    "                class_correct[label] += 1\n",
    "            class_total[label] += 1\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = [class_correct[i] / class_total[i] if class_total[i] != 0 else 0 for i in range(num_classes)]\n",
    "for i in range(num_classes):\n",
    "    print(f\"Accuracy for class {class_labels[i]}: {class_accuracy[i] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1cbb4c-1967-4198-b019-46b4aca56a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf0c90e9-2433-4f2b-b096-4bc7da959fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: Sesame\n",
      "Probability: 0.888537585735321\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the transformation for the input image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"C:\\Users\\mohab\\Desktop\\New folder\\tomato\\WhatsApp Image 2024-04-12 at 15.25.13_295d1696.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Apply transformations\n",
    "input_image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_image = input_image.to(device)\n",
    "model = model.to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    predicted_class = torch.argmax(probabilities).item()\n",
    "\n",
    "class_labels = ['Common wheat',\n",
    " 'Convolvulus arvensis',\n",
    " 'Cotton',\n",
    " 'Euphorbia peplus',\n",
    " 'Grass',\n",
    " 'Lolium multiflorum',\n",
    " 'Maize',\n",
    " 'Nutgrass',\n",
    " 'Purslane',\n",
    " 'Sesame',\n",
    " 'Sugar beet',\n",
    " 'Tomato']\n",
    "\n",
    "# Print prediction\n",
    "print(\"Predicted Class:\", class_labels[predicted_class])\n",
    "print(\"Probability:\", probabilities[predicted_class].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d591041-a46c-4ea9-8fc7-2b1fea5e3779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1196498-ed9b-4cf1-8038-2a3bcfae5ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2abdab5-879e-4c95-94ba-0d09bcdfa4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=0),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(0.25),\n",
    "    nn.Linear(in_features=9216, out_features=64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=64, out_features=num_classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "957504b5-2e1f-4ba7-8ab9-a218873a9bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (4): ReLU()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (7): ReLU()\n",
       "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (9): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (10): ReLU()\n",
       "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  (13): Dropout(p=0.25, inplace=False)\n",
       "  (14): Linear(in_features=9216, out_features=64, bias=True)\n",
       "  (15): ReLU()\n",
       "  (16): Linear(in_features=64, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c8ee41-7257-4225-bc06-f40bd3c69144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
