{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d482efd-e16d-453f-8765-bbbec4b6e758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████████████████████████████████| 1847/1847 [18:53<00:00,  1.63it/s, accuracy=89.3, loss=0.0666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6462, Accuracy: 97.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████████████████████████████████| 1847/1847 [18:38<00:00,  1.65it/s, accuracy=94.8, loss=0.0643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6572, Accuracy: 96.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████████████████████████████████| 1847/1847 [18:44<00:00,  1.64it/s, accuracy=95.8, loss=0.0639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.1181, Accuracy: 49.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████████████████████████████████| 1847/1847 [18:36<00:00,  1.65it/s, accuracy=96.2, loss=0.0638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6321, Accuracy: 98.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████████████████████████████████| 1847/1847 [18:35<00:00,  1.66it/s, accuracy=96.9, loss=0.0635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7156, Accuracy: 90.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████████████████████████████████| 1847/1847 [22:11<00:00,  1.39it/s, accuracy=97.4, loss=0.0633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7083, Accuracy: 91.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████████████████████████████████| 1847/1847 [18:36<00:00,  1.65it/s, accuracy=97.4, loss=0.0633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7364, Accuracy: 88.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████████████████████████████████| 1847/1847 [18:41<00:00,  1.65it/s, accuracy=97.5, loss=0.0632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6302, Accuracy: 98.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████████████████████████████████| 1847/1847 [18:32<00:00,  1.66it/s, accuracy=97.6, loss=0.0632]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6253, Accuracy: 99.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|█████████████████████████████████████| 1847/1847 [18:33<00:00,  1.66it/s, accuracy=98.1, loss=0.0631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7443, Accuracy: 87.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|█████████████████████████████████████| 1847/1847 [18:34<00:00,  1.66it/s, accuracy=97.9, loss=0.0631]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6553, Accuracy: 96.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████████████████████████████████| 1847/1847 [18:29<00:00,  1.66it/s, accuracy=98.3, loss=0.063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7104, Accuracy: 90.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|█████████████████████████████████████| 1847/1847 [18:32<00:00,  1.66it/s, accuracy=98.5, loss=0.0629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6247, Accuracy: 99.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|█████████████████████████████████████| 1847/1847 [18:33<00:00,  1.66it/s, accuracy=98.5, loss=0.0629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6528, Accuracy: 96.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|█████████████████████████████████████| 1847/1847 [18:30<00:00,  1.66it/s, accuracy=98.5, loss=0.0629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.7341, Accuracy: 88.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|█████████████████████████████████████| 1847/1847 [18:30<00:00,  1.66it/s, accuracy=98.7, loss=0.0628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6897, Accuracy: 92.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|█████████████████████████████████████| 1847/1847 [18:31<00:00,  1.66it/s, accuracy=98.5, loss=0.0629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6281, Accuracy: 99.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|█████████████████████████████████████| 1847/1847 [18:31<00:00,  1.66it/s, accuracy=98.7, loss=0.0628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6311, Accuracy: 98.77%\n",
      "Validation loss did not improve for 5 consecutive epochs. Early stopping...\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "train_size = datasets.ImageFolder(root=r\"C:\\Users\\aliof\\OneDrive\\Desktop\\faculity project\\processed resized data\\augmented ali data\\balanced_dataset\\train\", transform=transform)\n",
    "val_size = datasets.ImageFolder(root=r\"C:\\Users\\aliof\\OneDrive\\Desktop\\faculity project\\processed resized data\\augmented ali data\\balanced_dataset\\valid\", transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_size, batch_size=26, shuffle=True)\n",
    "val_loader = DataLoader(val_size, batch_size=26, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet101V2 model\n",
    "base_model = torchvision.models.resnet101(pretrained=True)\n",
    "\n",
    "# # Freeze the parameters of the pretrained model\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Get all layers of the ResNet-101 model except the last one\n",
    "all_but_last = nn.Sequential(*list(base_model.children())[:-1])\n",
    "\n",
    "num_classes = len(train_size.classes)  # Number of classes in your dataset\n",
    "\n",
    "# Define the extended model with additional layers\n",
    "model = nn.Sequential(\n",
    "    all_but_last,\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(1024, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Optionally, move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 50\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True) as t:\n",
    "        for inputs, labels in t:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            t.set_postfix(loss=running_loss / total, accuracy=100 * correct / total)\n",
    "            \n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    # Calculate average validation loss and accuracy\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model_paper.pth')\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f'Validation loss did not improve for {patience} consecutive epochs. Early stopping...')\n",
    "        break\n",
    "\n",
    "print('Training completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b082212-05dd-4b78-a82a-10ad3d6ffaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Modified:\n",
      "FLOPS: 7866.50 million\n",
      "Number of parameters: 44.61 million\n",
      "Number of trainable parameters: 44.61 million\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from thop import profile\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained ResNet101V2 model\n",
    "base_model = torchvision.models.resnet101(pretrained=False)\n",
    "\n",
    "# # Freeze the parameters of the pretrained model\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Get all layers of the ResNet-101 model except the last one\n",
    "all_but_last = nn.Sequential(*list(base_model.children())[:-1])\n",
    "\n",
    "num_classes = 11  # Number of classes in your dataset\n",
    "\n",
    "# Define the extended model with additional layers\n",
    "model = nn.Sequential(\n",
    "    all_but_last,\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(1024, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "\n",
    "#load our trained model\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Define input tensor shape (batch_size, channels, height, width)\n",
    "input_shape = (1, 3, 224, 224)  # Assuming input images are RGB with size 224x224\n",
    "\n",
    "# Move the model to the appropriate device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Generate random input data for profiling\n",
    "inputs = torch.randn(input_shape).to(device)\n",
    "\n",
    "# Calculate FLOPS\n",
    "flops, params = profile(model, inputs=(inputs,), verbose=False)\n",
    "\n",
    "# Calculate number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"For Modified:\")\n",
    "print(f\"FLOPS: {flops / (10**6):.2f} million\")\n",
    "print(f\"Number of parameters: {params / (10**6):.2f} million\")\n",
    "print(f\"Number of trainable parameters: {num_trainable_params / (10**6):.2f} million\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4adb735b-ecba-4b89-b19d-385d72672756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 42.51 million\n",
      "Number of trainable parameters: 42.51 million\n",
      "FLOPS: 7864.40 million\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from thop import profile\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load ResNet101V2 model\n",
    "model = models.resnet101(pretrained=False)\n",
    "\n",
    "# Modify the last fully connected layer to classify 7 classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(num_ftrs, 15)\n",
    "\n",
    "# Count the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total number of parameters: {total_params / 10**6:.2f} million\")\n",
    "print(f\"Number of trainable parameters: {trainable_params / 10**6:.2f} million\")\n",
    "\n",
    "\n",
    "# Create a dummy input\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Calculate FLOPS\n",
    "flops, _ = profile(model, inputs=(dummy_input,), verbose = False)\n",
    "\n",
    "\n",
    "print(f\"FLOPS: {flops / 10**6:.2f} million\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fd966c8-2733-404d-9089-b6078d6dae76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: Class Label - Common wheat\n",
      "Index 1: Class Label - Convolvulus arvensis\n",
      "Index 2: Class Label - Cotton\n",
      "Index 3: Class Label - Euphorbia peplus\n",
      "Index 4: Class Label - Grass\n",
      "Index 5: Class Label - Lolium multiflorum\n",
      "Index 6: Class Label - Maize\n",
      "Index 7: Class Label - Nutgrass\n",
      "Index 8: Class Label - Purslane\n",
      "Index 9: Class Label - Sesame\n",
      "Index 10: Class Label - Sugar beet\n",
      "Index 11: Class Label - Tomato\n"
     ]
    }
   ],
   "source": [
    "# Get class labels\n",
    "class_labels = train_size.classes\n",
    "\n",
    "# Print class labels with their index\n",
    "for i, label in enumerate(class_labels):\n",
    "    print(f\"Index {i}: Class Label - {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07da66fc-c745-4b6f-8bce-2a98577d5783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class Common wheat: 100.00%\n",
      "Accuracy for class Convolvulus arvensis: 100.00%\n",
      "Accuracy for class Cotton: 25.00%\n",
      "Accuracy for class Euphorbia peplus: 14.29%\n",
      "Accuracy for class Grass: 100.00%\n",
      "Accuracy for class Lolium multiflorum: 100.00%\n",
      "Accuracy for class Maize: 50.00%\n",
      "Accuracy for class Nutgrass: 96.43%\n",
      "Accuracy for class Purslane: 100.00%\n",
      "Accuracy for class Sesame: 100.00%\n",
      "Accuracy for class Sugar beet: 25.00%\n",
      "Accuracy for class Tomato: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#to test each class prediction accuracy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "])\n",
    "\n",
    "class_labels = train_size.classes\n",
    "\n",
    "# Load dataset\n",
    "val_size = datasets.ImageFolder(root=r\"C:\\Users\\aliof\\OneDrive\\Desktop\\faculity project\\test\\New folder\", transform=transform)\n",
    "\n",
    "# Create data loader for validation set\n",
    "val_loader = DataLoader(val_size, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load pre-trained ResNet101V2 model\n",
    "base_model = torchvision.models.resnet101(pretrained=False)\n",
    "\n",
    "# # Freeze the parameters of the pretrained model\n",
    "# for param in base_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# Get all layers of the ResNet-101 model except the last one\n",
    "all_but_last = nn.Sequential(*list(base_model.children())[:-1])\n",
    "\n",
    "num_classes = 12  # Number of classes in your dataset\n",
    "\n",
    "# Define the extended model with additional layers\n",
    "model = nn.Sequential(\n",
    "    all_but_last,\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(1024, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "\n",
    "#load our trained model\n",
    "model.load_state_dict(torch.load('best_model_paper.pth'))\n",
    "\n",
    "# Optionally, move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "model.eval()\n",
    "class_correct = [0] * num_classes\n",
    "class_total = [0] * num_classes\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if label == pred:\n",
    "                class_correct[label] += 1\n",
    "            class_total[label] += 1\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_accuracy = [class_correct[i] / class_total[i] if class_total[i] != 0 else 0 for i in range(num_classes)]\n",
    "for i in range(num_classes):\n",
    "    print(f\"Accuracy for class {class_labels[i]}: {class_accuracy[i] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b72456b8-a422-403d-9837-298da9d61287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for class Common wheat:\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1-score: 1.00\n",
      "Metrics for class Convolvulus arvensis:\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1-score: 1.00\n",
      "Metrics for class Cotton:\n",
      "  Precision: 1.00\n",
      "  Recall: 0.25\n",
      "  F1-score: 0.40\n",
      "Metrics for class Euphorbia peplus:\n",
      "  Precision: 0.67\n",
      "  Recall: 0.14\n",
      "  F1-score: 0.24\n",
      "Metrics for class Grass:\n",
      "  Precision: 1.00\n",
      "  Recall: 1.00\n",
      "  F1-score: 1.00\n",
      "Metrics for class Lolium multiflorum:\n",
      "  Precision: 0.97\n",
      "  Recall: 1.00\n",
      "  F1-score: 0.98\n",
      "Metrics for class Maize:\n",
      "  Precision: 0.67\n",
      "  Recall: 0.50\n",
      "  F1-score: 0.57\n",
      "Metrics for class Nutgrass:\n",
      "  Precision: 1.00\n",
      "  Recall: 0.96\n",
      "  F1-score: 0.98\n",
      "Metrics for class Purslane:\n",
      "  Precision: 0.33\n",
      "  Recall: 1.00\n",
      "  F1-score: 0.50\n",
      "Metrics for class Sesame:\n",
      "  Precision: 0.64\n",
      "  Recall: 1.00\n",
      "  F1-score: 0.78\n",
      "Metrics for class Sugar beet:\n",
      "  Precision: 0.11\n",
      "  Recall: 0.25\n",
      "  F1-score: 0.15\n",
      "Metrics for class Tomato:\n",
      "  Precision: 0.00\n",
      "  Recall: 0.00\n",
      "  F1-score: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define lists to store metrics\n",
    "class_precision = []\n",
    "class_recall = []\n",
    "class_f1_score = []\n",
    "\n",
    "# Evaluate the model on the validation dataset\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate precision, recall, and F1-score for each class\n",
    "for i in range(num_classes):\n",
    "    precision = precision_score(true_labels, predictions, labels=[i], average='micro')\n",
    "    recall = recall_score(true_labels, predictions, labels=[i], average='micro')\n",
    "    f1 = f1_score(true_labels, predictions, labels=[i], average='micro')\n",
    "    class_precision.append(precision)\n",
    "    class_recall.append(recall)\n",
    "    class_f1_score.append(f1)\n",
    "    print(f\"Metrics for class {class_labels[i]}:\")\n",
    "    print(f\"  Precision: {precision:.2f}\")\n",
    "    print(f\"  Recall: {recall:.2f}\")\n",
    "    print(f\"  F1-score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee797b30-dd5e-439e-b57a-2f9867ec2b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
